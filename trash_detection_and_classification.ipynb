{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a4ac6dd-ce04-4c73-b873-f2d98d0f4259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\user/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-10-18 Python-3.11.3 torch-2.5.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "C:\\Users\\user/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        xmin      ymin        xmax        ymax  confidence  class  \\\n",
      "0  14.563878  5.281422  478.322784  565.548035    0.752731      0   \n",
      "\n",
      "                     name  \n",
      "0  general wastetrash bin  \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649ms/step\n",
      "Processed image saved as c:/bin/processed.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "\n",
    "# Load the fine-tuned YOLOv5 model (from the location of the trained weights)\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'custom', path='C:/yolov5/runs/train/exp6/weights/best.pt')\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model (for full/empty classification)\n",
    "classification_model = load_model(r\"c:/bin/trash_bin_model_epoch10.keras\")\n",
    "\n",
    "# Function to detect trash bins using the fine-tuned YOLOv5\n",
    "def detect_bins(image):\n",
    "    # Detect objects in the image using YOLOv5\n",
    "    results = yolo_model(image)\n",
    "    boxes = results.xyxy[0]  # Extract bounding boxes# Detect objects in the image using YOLOv5\n",
    "    print(results.pandas().xyxy[0])  # Print detection results for debugging\n",
    "    return boxes\n",
    "\n",
    "# Function to classify bins (full/empty) using MobileNetV2\n",
    "def classify_bin(cropped_image):\n",
    "    resized_img = cv2.resize(cropped_image, (224, 224))\n",
    "    img_array = np.expand_dims(resized_img, axis=0) / 255.0  # Normalize\n",
    "    prediction = classification_model.predict(img_array)\n",
    "    return prediction[0] > 0.5  # Return True for full, False for empty\n",
    "\n",
    "# Function to process an uploaded image\n",
    "def process_image(image_path):\n",
    "    # Read the input image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Detect trash bins using the fine-tuned YOLOv5\n",
    "    boxes = detect_bins(img)\n",
    "\n",
    "    # Create a copy of the image to draw bounding boxes on\n",
    "    output_img = img.copy()\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box[:4])  # Get box coordinates and cast to int\n",
    "\n",
    "        # Crop the detected bin from the image\n",
    "        cropped_bin = img[y1:y2, x1:x2]\n",
    "\n",
    "        # Classify the cropped bin as full or empty\n",
    "        is_full = classify_bin(cropped_bin)\n",
    "\n",
    "        # Draw rectangle and label\n",
    "        color = (0, 255, 0) if is_full else (0, 0, 255)  # Green for full, Red for empty\n",
    "        cv2.rectangle(output_img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        # Add label (General Waste/Plastic) and Full/Empty\n",
    "        label_text = f\"Full\" if is_full else f\"Empty\"\n",
    "        cv2.putText(output_img, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    # Save the processed image to a file\n",
    "    processed_image_path = r\"c:/bin/processed.png\"\n",
    "    cv2.imwrite(processed_image_path, output_img)\n",
    "    print(f\"Processed image saved as {processed_image_path}\")\n",
    "\n",
    "# Test the process with a sample image\n",
    "process_image(r\"c:/bin/new_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85e324-86da-4da1-ad16-4c9571a554ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2540ca9-298f-4bfd-bbd9-278f65fa0a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a23e7-156f-4db7-a050-8d55353312da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040521ef-a59c-430f-b24b-b3a1dfa8f66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
